This talk is about a specific project that we did at Flipkart.

So just to give people, like foreigners here, a bit of context of Flipkart: it's sort of like the Amazon of India. They are the largest e-commerce store in India. 

So yeah, let me keep the numbers for later.

In 2011, around about in December, we had a moment where we realized that our supply chain was not going to be able to scale. We were having very, very serious problems with actually building new features. 

It was going very slow. The number of requests hitting our website was just through the roof and we were not able to keep up. At that point there was a...

So let me just actually talk about what the system looked like. This was Version 2 of the supply chain at Flipkart. Version 1, which was written in 2007, was written by our founders, Sachin and Binny themselves, in PHP. 

This was Version 2, which was written in about 2010. It was Opensource ERP system called Opentaps, which we had extended extensively. It was basically a single monolithic application. It had all these modules for the management, warehousing, so on and so forth. 

They all came out of a single JVM and connected to a single database. And that was just horrible.

The problem was that each module, as we were extending it, the developers didn't actually bother to think about, should we be accessing this data, this table. Should we be crediting it or not? They would just go and make joins across tables, just to solve the problems - get the features out.

You're probably familiar with that.

So, horrible coupling. And we spent about a month trying to see if we could call up each piece from the system and kind of start picking out services. We couldn't do it. It was just impossible.

So at that point we took a hard, hard decision that, you know, let's actually rewrite. And this was something which was completely against (my past works?) philosophy of, you know, let's re-factor incrementally and go slowly. Let's have the system running and then kind of migrate. But we took the score, and in 2011, in December, we started the project.

So this was a sort of bet the company project for Flipkart. It was so critical at that point that Sachin, he was the founder, actually came and sat with the team. So we basically called up a team of initially ten people, and then that increased to about thirty developers.

He moved us, moved that team out to a separate office, which was basically a house in Banglore, which was the place where Flipkart was born. And that got turned into a ?? work start-up project within Flipkart, where this team worked in complete isolation. No interviews, no meetings, nothing.

This team was only here to build out this system in seven months, because the next milestone was the Diwali.

The Diwali is the time when we do the most sales in the year, and that was in October of 2012. We had about seven months to replace an entire supply chain system with a new system built from the ground up: get it in production, make sure it's working, make sure it's scaling right.

So get to it, probably do it by August, and give ourselves time till August to kind of do that.

We start up on this project and the idea was to break up each of these modules into services. And I think Chad's talk really set up the context beautifully for the start, because a lot of the ideas and processes that he mentioned is stuff that we tried to kind of work on and kind of implement in the system.

So, some of the things he wanted to do was - I think it was nice because it's called small pieces loosely joined, which I came across around that time. I think that beautifully summarizes what we want to get to. Each service doing one small thing.

So you break down the warehouse module into a separate service, order management into a separate service, accounting, and so on and so forth. 

We didn't want to go down the micro-services architecture way. I'd worked with Fred George ealier and I'd gone down, I'd seen the downsides of it and I didn't actually have a clear idea of how to work around those downsides at that point in time. So I was kind of wary about micro-services at that point, and I would love to actually hear other peoples' thoughts on how that's working out.

But anyway, so we all took on our separate services, each doing one thing and doing one thing well. And each service would have its own database and nobody could access it except the servers. 

You could just access it through an HTTP JSON API and you will never touch my data. My private parts are private. 

So this is what we ended up with. You're probably not going to read the thing so I'm going to read it out to you.

We ended up with about twenty-five services. This is a sub-set of the services we actually built. You have all the management service, then the fulfillment orchestration service, and fulfillment, which in turn talks to supplier and the whole logistics subsystems. 

And you have accounting services, document services, and then you have a bunch of infrastructure services including this piece at the bottom, which was a messaging system that we ended up building called Resbus, which I'll talk a little about, which kind of addressed the problem of cross-service transaction integrity.

So in this picture the Ruby services were basic- so, each service, for example, the order management service, the blue pieces were written in Padrino or Sinatra. So these are in Padrino, and we added JRuby when we went live. We eventually migrated those to MRI, and I'm going to be clear about why.

So these were the Padrino services, and then the UI pieces that were required were written in Ruby on Rails running on MRI. We also had some infrastructure services running on Ruby, for example the single sign-on service was, and used CAS and used RubyCAS.

We built our rule-based access system again in Ruby, and a bunch of other pieces.

So essentially went went from a monolithic single system to twenty-five services, and this was a massive change. 

There were total about seven teams which worked on this project, owning one or two serives. Each team had between four and six developers. 

So that's just to set the context.

When we started the project in early 2011, the old system was doing about 20,000 orders a day, 30,000 shipments a day, roughly around that order. This new system has now survived two Diwali workloads, which includes the latest Diwali which was in 2013. We did, I think, 100,000 orders and 150,000 shipments. It's working pretty well.

At the time when we were making this decision, we were kind of selecting the technology stack and the big question was which stack to use.

So Flipkart traditionally had a lot in Opentabs given the Java stack. So most developers were very, very comfortable with Java. The thought of actually introducing a new language, a new ecosystem was... People were kind of wary about that.

Also, there were concerns about performance. But I knew from my experience in the past that performance is not a language issue. It's an architecture design issue fundamentally. And there are differences in technologies and languages, and I'll talk a little bit more about that in detail. But performance I'm not too worried about.

So why Ruby then?

The obvious reason is speed of development. We had a very, very tight deadline to detail the entire ?? system in seven months. We wanted to move fast. So that's obviously one benefit.

The other reason was... I think this idea of small pieces loosely joined is really powerful. Small code pieces are inherently easier to debug. Even if you have the best tools and you have a large Jar system, it's hard to work even if things are great for filers [and you have] great modeling tools. It's still hard when you are dealing with a 100,000 line code base. It's just far more complex than dealing with something that is maybe 10,000 lines, or maybe 5,000 lines of code.

And that code compression that Ruby allows is actually really powerful. Also I think in the Ruby world, Ruby's secret weapon is not Rails. I think, to me, Ruby's secret weapon is Activerecord, especially for business applications.

I just love it as a new item tool. It has its problems, especially in the enterprise space when you are dealing with fairly complex logic. Because it lacks what's called an identity map you can have situations where you're not traversing from parent to object, to a child object, and back to the parent object, you end up with two references to the two instances of the parent object. And that's kind of bad.

Tools like Hibernate in the Java world actually solve this beautifully. So yeah, it has its glitches.

But still, as a tool to write business applications, it's fairly amazing. 

So we want to use the power of Activerecord. We want to have small systems, which is why we built our services, back-end services, which were the HTTP JSON ones, only in Sinatra. So Padrino was just a pin wrapped around it. So it's Sinatra talking to its own database.

I'm going to try and focus a lot more on the lessons learned, so I'm kind of going to be jumping topics a bit and there might be a bit of discontinuity. You'll have to excuse that, but I want to kind of get the real key insights that we had on the project out than worry about a kind of consistent flow.

So JRuby.

Let me start it off good. JRuby is incredible. It's an amazing piece of technology, great community. You get the power of the JVM which is just amazing, and specifically within the JVM what you want is its garbage collector and the just-in-time compilation. 

Those two are just amazing. 

I'll share some numbers about how those two things actually make a difference.

So that's a good part of JRuby, right. Amazing ecosystem. You get all the tools that are in the Jar that would just work, not just work but work OK, in JRuby context. Still you get a lot of tools like this that can be used, et cetera.

The bad. 

So what's bad about JRuby?

One thing that gets talked about a lot is its slow start-up time, and it is a massive, massive issue. Typically, when you're kind of coding, particularly when you're testing, you want to have a very quick cycle of going from test to code to test and back and forth. But that's hard to do with JRuby.

There are other things you can do. You can use things like ??, is almost like using Nailgun, for example, or Spork. 

--?

And then connect to it and run tests against it. So all that is fine but it's still very sluggish to work with.

So you end up having to kind of jump through hoops to work good on that problem. 

So for example, all our development was in CRuby. Tests run fast, specs run fast, scripts run fast. But you deploy to JRuby, and even then, even for CI and for deployment, you end up isolating the kind of scripts which were launched in CRuby. But they were in turn just launched in JRuby just for the pieces which actually required JRuby. So you had to kind of do a bunch of these things.

Just kind of not great.

But all this you could kind of live with. There's one thing about JRuby which suprisingly isn't talked about so much, which I think is fundamentally a deal breaker, and that's its test suite.

It's not actually a JRuby problem.

I just think the Ruby world is just not ready to work on a truly multi-threaded Ruby interpreter, which is without a global interpreter lock. And this manifested in horrible, horrible bugs for us.

So when you start working at scale, you know you're getting tons of requests in the system. These problems don't typically manifest when you're running a small service, and you know JRuby works fine for that. 

But you run into cases where some library was not written with thread safety in mind and that will just kill you, and it is virtually impossible to debug.

So the beefiest problem with Padrino is where the actual app wouldn't get initialized and it was just horrible to bring it out. And the problem turned out to be something in the HTTP router, which is a gem used for actual route of creation. There's no fix for that. It's been over a year and a half and it's still not been fixed.

We actually put in a patch to Padrino to actually work around it. We cleared the rack filter, which kind of hand-holds the initializing process and initializes each BM correctly. 

It was just horrible, horrible, horrible code. 

HTR is one example. We were [using] Activerecord. We were on 3.1.X. So Activerecord has got concurrency, has got concept issues, and they don't show up, again, in normal situations. They show up at scale, at high load. The connection pool is actually not thread safe.

So we had situations where the same connection was being returned to two different threads and they would end up just messing up the data base or the transaction. So essentially what would happen was the transaction would commit, the service would say OK, 200, all OK, committed, and you would go back and see there's no data in the database because the transaction never committed.

The connection was basically rolled back at some point and nobody knew anything about it. So this was horrible and we couldn't figure it out. We tried patching, we've actually patched Activerecord quite a bit, but we couldn't get that to work, so we actually went live with the JRuby in September of 2012.

I think in three months after kind of struggling with these issues we kind of threw a call to move back to CRuby and that was kind of a sad moment because I really, really loved JRuby and the great joys of people using it in production. Not too many, but yes, there are. 

Its potential is there but the problem is the libraries. And developers are still not in that Java mindset. Surprisingly Java does this very well. They're kind of constantly thinking about thread safety, but the Ruby world is still not part of that.

They'll probably get there soon.

So we moved to CRuby and that sorted out a lot of performance issues, rather, thread safety issues. I'll talk about the performance parts actually. I'll compare.

Also besides performance, I'm guessing you're saying, but that wasn't too much of an issue.

So I mentioned this briefly and I'll just touch on it.

So what's a problem here? When you have a bunch of services like this, when you have a single monolithic application, you have a request which comes and kind of touches multiple databases or the management warehouse, and you can run that, all those database changes in a single transaction. It commits or doesn't commit. Everyone's happy - all beautiful. 

The moment you go to something like this, that doesn't work right. Supposing how a transaction which, say, called create_order, which kind of enters the create_order request, which comes through the order management system - it basically makes the call to approve that order. The fulfillment of strata and as part of the fulfillment orchestration basically tells the warehouse service that they don't have the stuff in stock. I'm going to actually order it for you.

So it tells the procurement service to go on, order this item. It also has to tell warehouse that, OK, expect this item, I'm ordering it for you. Expect it.

Now these two things, which is placing the order with the supplier and expect the order, has to happen automatically. They have to happen automatically along with the commit of the approval.

OK, so it got approved and I've told this guy to procure it, and I've told warehouse to expect it. I have these two things happen at one time. Now, this is a really, really hard problem to solve.

The way you solve that typically is to, in the enterprise world, use the stored transaction call data. Most of the ?? services, for example, do that.

They kind of implement the two face commit, and you can actually have a database - two databases - take part in a single transaction or two phases. That's one way to do it.

The problem with that is that completely breaks service isolation, like now I've got a distributed transaction coordinator which is going to be sitting and coordinating transactions between this database and this guy. And if you remember the original principle, I don't want to expose my database.

I want to expose my service. So it kind of breaks service and translation very badly, and that's the least of the problems. The bigger problem is that it degrades. It's basically a way to ensure the system doesn't scale.

Because when you have a transaction spanning multiple system, what's happening is happening under the hood for each database. The resource packages require some locks on some rows in the table and those are now held for a much longer time, because you're going through two passes through it.

So essentially you end up holding locks on database rows for much longer, which increases contention and reduces book problems.

So then how do you solve this problem?

The way you solve it... The other option is to actually go in for something like, use massaging. You actually send messages to other services and the push to the message cue and the right database has to happen as one transaction.

So that's another option. But again you need a distributed transaction coordinator to kind of manage the two face commit coming between the message cue and the database.

What we end up doing is view-

(interrupt) Are you serious? Ten minutes? All right.

We end up creating a service called Resbus which kind of, it actually does local transactions and asyncronous relayer of messages to actually call upon. I can talk offline about that in more detail if anybody's interested.

So what we learned there though was this: the power of HTTP as integration glue was just understated. You end up creating, using messaging systems and use databases.

Why do you need that? Why can't messaging be exposed as HTTP inquiry?

We did that and actually that had amazing side-effects to viewer architecture. 

Ruby's performance is often talked about as being sluggish. Let's kind of get an intuition for how good or bad that is. Let me ask you a question.

If you had a hello world Sinatra route and you were to pound it with requests, what kind of response sends can you expect?

It's just a hello world, so just a get slash hello world, and it just says hello world and returns. 

Some guesses of how long that would take?

5 milliseconds?

Yeah.

It takes about a millisecond at the 95th percentile, but it takes nine to twelve milliseconds at the 99th percentile, and the max is around fourteen. If you run the same thing in JRuby, it will take about 2 milliseconds at the 95th percentile and the 99th would be about 5 milliseconds. It's actually very, very tightly bound. 

That's the beauty of JRuby, because it's GC is so good and the zip compilation is so good that it's able to clearly optimize pieces of code to give you very, very stable response times. 

But they're actually worse than MRI. So the bench marks that talk about JRuby being faster, I've never been able to reproduce those. 

Anyway, that's an intuition... We end up getting much higher support. So that same hello world server will do about seven hundred requests per second versus the JRuby one, which will take about five hundred fifty to five hundred and eighty.

However, if you had a tomcat servlet doing hello world, how long do you think that would take? Thirty?

Yeah, it would be roughly fifty microseconds, it's about twenty to forty times faster. So that is one of the big arguments that was made against using Ruby.

Why should you use Ruby, it's so slow.

But the point is this: it's fill fine, perfectly fine, and that is because most business applications are not CQ-boned, they're IO-boned. They're basically all just waiting for some horribly slow query to return, and it takes time waiting in Java or in Ruby.

That's why actually IO in managing, IO is the most important thing. That includes things like database calls, calls to external services, kind of optimizing that is the first priority for tuning Ruby apps. So given this we wanted to make sure that all our services were kind of behaving well.

We actually built a tool called drac metrics, which basically is a rack filter which will send the request cycle and you can actually add plug-ins to monitor different parts of the application.

We had to use plug-ins for Sinatra's routes so you can get into it and request time. 

We had plug-ins which can go to Activerecord and calculated the time for each query.

It hooked into the desk client and thrift clients to basically instrument the time taken for all our outgoing calls. The result was it would - oh look, you can't read it ver ywell - essentially you'd see you could go to any service and ask for its metrics. 

It'll tell you all the routes that were defined, all the routes and how much total time was spent in them, the total count - we have average time and max, et cetera. And within each route, if you expanded it, it would tell you the five slowest requests for that route.

So for example, for this inventory post, inventory call, there's the caller tells you how much DB time was spent, how many rest calls, how much rest calls, was split between different parts of the code. It'll give you a list of all the external calls and database queries made.

With this, you could immediately find out that you're doing something stupid like an N plus 1 query, or the call to external system to actually slow, and that kind of optimized.

You have a check list for kind of figuring out what you want to attack first. So we'd use this, figure out the slow points, optimize that first. We also had an extensive monitoring through StasD and Graphite. 

Graphite allows you to kind of just push points to it and it gives you a time series of the data. And we've got, again, as Chad was saying, like metrics from business down to tech metrics including CPU and capacity and request response times, number of requests. 

With this we also built a loading framework which basically models Graphite, and you can fine if this threshold is breached for this metric, then send me a mailer SNS. With these three pieces, we had the modeling in place to kind of keep a check our systems.

How do you tune that?

Once you kind of figure out the problems that are typically there it's actually fairly straightforward.

If it's IO, fix the IO problem. If it's N plus 1 query, remove it, do an eager join - simple stuff. If it's a bad query, run MySQL Explain or whatever query plan or tool you have for a database. 

You get the query plan, figure out what the optimal join struct- join sequence, and actually fix the query.

For external cause, we actually end up putting in very aggressive time-outs so that you don't have a thread or a process just waiting forever for somebody else to respond. Because that has very bad effects on capacity of the entire cluster. Because, for example, if you have a service that is calling another service and that service is running slow, you can end up completely freezing this calling service, because all the processes are just waiting for this guy to respond.

So once those two kind of things are fixed, then it's important to look at GC itself. 

Now Ruby's default GC settings are actually very conservative and you typically see initial after restart. Response times are kind of slow and they get kind of faster, they can work pretty fast but there's still lag. 

There are ways to kind of improve the default heap size and the allocation rate and percentage and all that stuff. 

I've got a reference to that at the end.

I think that there's something which we did and that immediately improved our response times. This still doesn't address all problems. There are still cases where GC is still a problem.

For example, there will be queries which just load a ton of data. Say, for example, reporting queries, and those are tricky. For those what we end up doing is just carving out a separate clulster for the recording queries, build a separate virtual IP, a separate rip, and point all those requests to those servers.

Even if that doesn't fix the problem with GC, then you've got to go down to profiling.

So for profiling we end up using both tools - again there's a URL link to it at the end - which is a great, great tool. There are rack filters which can kind of mod the request of, hook into the request cycle and actually give you a grasp of the call-chains, along with how much time was being spent in each section.

That's great. And that'll tell you how much time is spent in GC or some section of code. So that's your final, final kind of hammer you can kind of use to addrtess performance issues.

We also ended up clinging on quite a bit with the changing app source, so we moved from Trinidad in the JRuby world, time, to MRI views, passenger first, and now we have been on Unicorn.

Unicorn particularly has a plug-in called the out-of-band GC which basically runs GC on that, basically disabled GC for that particular interpretator and runs GC after the request cycle ends. 

So all your response, none of your requests actually see bad response times due to GC. That works beautifully actually. Really nice plug-in.

There's also a plug-in called workerkiller which can kind of get a look at the process of the memory threshold. 

We use all these.

Another problem that we faced, which was that we now have twenty-four, twenty-five services running on Sinatra and Padrino apps, and there was a bunch of Ruby gems which had to be shared and each team worked independently so it ended up becoming a problem for the platform team to kind of go and chase people around, say, create this gem otherwise there's gonna be problems.

That was a big issue.

So what we ended up doing was creating a patch version of Bundler, which essentially allows you to annotate each new gem file, each gem line with an auto-updated coded true flag, and then Bundler basically will, when you run Bundle install, it will basically check if there's a new version of that same gem in the repo, and actually resolve dependencies and actually install that.

The nice thing with this was, it works in the debs environment. It's not something that happens automatically in the background, you're weighing it, kind of messing around platform gems. It happens frequently enough that you know everybody, the entire ecosystem kind of moves together and upgrades the services quickly.

(interuption)

Can I take five minutes? No? All right, I'm, OK, I'll rush through.

Team dynamics.

We saw with the Java team, and we kin of just threw them to the deep end into Ruby, and we just had to work three Ruby debs, so it took, on average I think, people about three to four months to start writing idiomatic Ruby. That was a big challenge.

We did a bunch of things to kind of address these problems, uncluding kind of having consultants to bear with the people at the time, having on expert per team, et cetera. OK, I'll skip to... all right.

So one of the things that we noticed, or happened was that because Ruby's such a dynamic language, you can write so much code so quickly and so compactly, sometimes it tends to, designs tend to b taken for granted.

I don't know if it's automatic or not, but it tends to be taken for granted.

Like people don't, just don't think, like, OK, there's a feature, then we decided quickly, yeah, let's not try to set it on them then. But I think this is a really big problem because you end up, or rather you need to actually ask very deep questions about the domain, and we actually got caught in this problem a couple of times where we actually ended up creating small custom solutions to specific problems instead of asking deep questions about it.

What is this domain, what is the problem really about, what is warehousing, for example, really about. It's not about check lists and foot lists, it's about goods movement and material movement. And how do we model that as a first class concept?

I wonder if Ruby being a dynamic powerful language actually cultivated that. That's more of a question for me.

OK, yeah, questions. The slides, the references at the end.

BACKGROUND VOICE: Sorry, you will get no questions.

YOGI: All right, no questions. Thank you guys.

BACKGROUND VOICE: Thanks for the insights, Yogi.
