Garden City Ruby 2014 - I've Got Your Number: Machine Learning in Ruby - Arnab Deka

Hey guys. Good morning? Yeah.

So I'll talk about machine learning. The promise and all that was just to get the CFE expected, so - accepted, so. Don't expect too much.

Anyway, so I started with machine learning about six months to a year back, and this world like amazes me with how much stuff you can do and how awesome it is. So at the end of this talk I just want you to get excited about that too and go and try it out.

You can read more about me there. Don't want to talk about that. The slides and the code that I'll show are up on GitHub and slides and all that.

Yeah, so I was preparing this thing and then Dejas, yesterday - everybody knows Dejas here, right? He had a couple of jokes that were exactly what I had, and I was like oh god I have to prepare new jokes now. But at night I couldn't so, it was too late anyway. So I didn't, so my jokes are still lame, but still laugh like you did for his talk, all right.

First of all, happy new year. Has anybody ever had two hundred people shout happy new year to you at the same time? No? Want to try that? All right, so, on a count of three, right.

One, two, three.

AUDIENCE & A.D.: Happy new year!

That was actually much better than what I expected. Cause I had two more steps after this. I'll just do one of them anyway. Let's try even better, right. Let the hotel know that we are doing something awesome in here.

So shout happy new year and stomp your feet. Don't worry, the floor is pretty strong, except probably this part. So, all right, like this. Three times, all right.

So one, two, three.

AUDIENCE & A.D.: Happy new year!

All right. You guys are already awake I know but, anyway. So we'll take a very brief look at machine learning. And there are many awesome talks that tell you what machine learning can do. This is not what this talk will do.

I'll specifically focus on classification, which is one type of machine learning, and an algorithm that you can use to do that, which is SVM.

And then at the end we'll see how you can do this in Ruby.

So yeah. This is my first talk, and I thought I should do a live demo. Right? Before we actually go- so, what we are trying to do today is, we have all these large data set of about 60,000 images that look like that 6 and 5 and 7.

Each one is 28 by 28 pixels, and what we have is raw gray scale intensity. So you have zero to 255. And the, there's this data set available that you can use to practice your algorithm on, or train your algorithm on.

So it's hard to read, it's all binary and I don't understand binary. So I put it out in CSV like this, and as you can see most of the pixels here at the top, bottom, sides, they're all going to be zero, and in the middle you'll see some numbers that's going to range between zero and 255.

So what we're trying to do is, take these 60,000 images, basically create, have a algorithm learn those images, and then when we give it a new input, like another image, that sort of looks like one of these, or maybe a little dissimilar, it should be able to tell us what it is, right.

So that, the MNIST set also comes with 10,000 things for testing. So first we'll run it through that and see what our accuracy is, and then I'll show you some of my bad, bad handwriting. All right, so.

I will talk about most of these things later, but I'm basically - you don't need to know what linear is, I'm just showing that I'm doing it on 60,000 samples, and I won't do the all 10,000 right now I'll just do 500 of them.

And that's going to give, come back and say hey I'm 85% accuracy and I didn't train the algorithm right now because that takes a lot of time. I already have it saved. And you can see the code and all that, the saved models in GitHub, but for now the main important parts are this algorithm, it says it's 85% accurate, on the 500 samples that we tested.

And the, all these things that you see, like for example this guy, we got two images that are actually 9, but are reported as zero. Right, so those are bad. Everything that you see along the diagonal are good, everything else is bad. And I didn't tune the algorithm at all because I wanted to keep it simplistic and all that, but you could do a lot more stuff and as VM I think it has been shown that it can get about 93% accuracy with this.

All right, so. Another thing is, so I wrote down some of these numbers on a sticky note. That's probably not visible because it's all white. I'll see here. Right. So.

Something like this, right. They wrote it on sticky, and then scanned it in gray scale. Then I'm trying to get it to read it from my own handwriting.

So I did, some of this Rails Magick stuff. Who's familiar with Rails Magick? Right, so I did some of that stuff to get the raw gray scale intensity and all that and so I make my picture look a little bit like more gray scale. Right.

So yeah, I'm going to use this kind of - say zero is pretty easy for an example because not many shapes are like that. So let's see if it knows what's written. It says the answer is zero. That's good.

And I'm going to open this so I know which ones are working and which ones are not.
[^ 00:06:09-00:06:12 - ??]

The 1 is working. And some of these are not working. For example I'll show you a 3. Look at it on the tables. That's 9. And then we'll get the 9. The way I write 9 is very similar to 3, except this part, right. So I'm going to try 9 and it's gonna tell me that it's a 3, right.

So that's when you need more accuracy to train your algorithm, right.

Anyway, I hope you sort of understand some what you can do with machine learning. So what we're doing here is, we're not explicitly programming the learning algorithm. We're just giving it a lot of data, and it automatically figures out how to solve a problem. Given unknown data.

Which is, I thought, amazing when I learned it. And it's basically statistics-driven algorithms to find patterns in data.

So there are two types of machine learning algorithms. The more widely-known ones are probably [00"07:12] house?? like supervised learning, where it has lots of data. For example, let's say you were doing house pricing, right. You have to have the area in square feet, you know the location of the house, you know how many bedrooms it has. Is it an apartment, does it have a swimming pool - things like that.

These are all attributes of our problem. When you know these attributes, you can basically, and without some set of data where you know the prices, you can now feed it to an algorithm, and when you have, I don't know, like, it's just some made up a two bedroom apartment, throw that in the algorithm, what should be the price?

And it can give you the price, right. Of course it'll need a lot of work, but that's essentially what supervised learning is.

You are given some known data, and then you predict unknown data.

Unsupervised is, on the other hand, there's no, like, classified data or something like that. You just give it a lot of data, and you're trying to find patterns in that. And for example, recommendation engines, like Amazon or Flipkart - everybody - recommendation engines, right, they go do something like that.

Now Flipkart does do something like that [00:08:15]?? I'm guessing.

And they do finding groups or certain groups within your social network, things like that can be learned.

So today we'll just talk about classification. And, so that example I talked about house pricing. Let's say that we'll need one attribute, we're gonna, we can barely sample here. And can everybody see this side, or should I be talking on the other side of this room? This side is fine? This - that side?

All right. OK, so, just one attribute we are focusing on, and I'm plotting all these houses. So this much area, this is the price, this much area, this is the price, sort of something like that, right. And this is not exactly linear because there are other attributes. We are just considering one attribute.

And we have this red dot here that we want to predict the right price on. What do you think is the best way to do that?

AUDIENCE MEMBER: Extrapolation.

A.D.: Right, extrapolation. So this is a linear extrapolation, and it's finding a linear thing, the progression will give linear data. The dots will be here, there, and there, and in that case you look data up on the only other fitting curve. In this case it's a linear curve.

And then once it do that, you figure out the dot will probably lie somewhere here. So it's running a price over here.

Which is not bad. Given the limited sample here?? [00:09:32] So how do you actually solve this problem? How do you find these dots that are away from the line? The thing you do - before I show you the math, for every one of these points, you say that if I knew what the slope was, how do I compare it to this?

So that's the lead hypothesis, right. Hypothesis on every dot is going to be this line equation here. You probably know this as V and this as M, right. V, V, V plus Mx. But I'm probably gonna use the theta one because you can have more than one attribute. You actually missed one. So that way you can rotate out, quantate and filtrate one, blah blah blah [00:10:20]??.

So you can't believe the hypothesis. Well right now you don't know the answer on data alone, right. So you know that Y there means is the actual price. So you take the Y and you find the difference between the hypothesis and the Y, while not knowing the hypothesis, right? So you're just forming the formula. And then you try to minimize that part of the whole picture, right.

So what you do is a declaration for every point. You take the Y out, square it, because, and then it's absolute, and then you find the sum of it. And this is your minimization problem.

How many of you have heard about linear programming, or optimization minimization. Yeah, so this is basically a minimization problem, and I know this, we'll figure out what equals zero and we can go on [00:11:10]??.

And this was a linear thing, so linear, but [00:11:14]?? we can also have polynomial curves.

Next up is, that was linear, so the price is a real number, so you're trying to predict the exact numbers, whereas this kind is saying is my- is this email spam or not spam?

And this is a very famous machine learning thing that Gmail?? [00:11:33] started. So I'm going to keep it simple, just considering two attributes of an email. Length of the email, and does it have a dollar symbol, right.

So based on those two attributes, I have plotted out, let's say something like - the red stars are spam emails. The blue ones are non-spam emails.

Now how do you get an unknown email, how do you classify whether it's a red star or not? It's very similar with the previous problem. You are again going to find the curve that's sort of a different, there's a difference between the red stars and not, and this one thing you do by computing some of these here [00:12:12]?? which both will show that this is probably not exactly a linear solution.

You'll need better than one attribute. So like the previous one, we are going to need a hypothesis. This time we need the hypothesis to be between zero and 1. So zero means it's not spam, and it doesn't matter what it means as long as they're consistent.

So the Y value will be either zero or one, so the hypothesis is also zero or one. And this is a very famous sigma function that I will deliver to you after you figure it out, but this for me ?? [00:12:46] results??.

And here, like the previous one, we're just using theta zero plus theta 1 in base form [00:12:53] right. This side, that whole thing, is Z and Z here. And then you solve that minimizer equation I guarantee you'll find equals theta zero or theta 1, and then you find that you are right where you should be.

So it will come somewhere between zero and 1, and you take that from. That's how you classify it.

Right. And how you exactly minimize those problems is, SVM is a powerful way to do that. Along with numeric constants [00:13:21] are widely-used in machine learning and [00:13:23]. It's called a large margin classifier, and the full-form stands for Support Vector Machines. We'll see what [00:13:30] means.

When I started, I thought, why are these things formed the way they are here? So again, red stars and green squares and we want to segregate them. We want to find the line that divides them. We can think of many lines. There's a pink line here, there's a light blue line there, then there's a deep blue line.

So, what SVM does is it finds all the lines that it potentially can use, finds the critical points that matters as the distance between the [00:13:58] line and the blue??, right.

So in the case of the pink one, this is the closest one for this side, and that's probably, let me identify it, so one of these points is the closest - so this, the closest one, is called the support vector for that group, right. All these other points, the difference between the pink line, and all these other points don't matter, because they are the minimal distance from the group.

So it finds out the line that best segregates these things. And this is the support vector, and this is the margin, the length of that is the margin, hence, large margin classifier, right.

So that's SVM, we are going, it will get exactly the algorithm [00:14:45], right.

So, and if you're feeling that, like that, it's OK, because this is about six times the [00:14:54]??

But I know a lot of you love coding, so let's slow down and get coding.

Right. So before we start coding, one more small theoretical segue. We learn how to move zero or 1, then do plusses, right. Spec or not spec. 

How do you do when there are multiple plusses? So in our example, we have ten plusses - the numbers zero through 9, we need to find out which number is it, right.

What you do in this case is, we have three classes here. First we'll read it three linear - three classification problems. The first one is whether it's a red star or not. That's going to give you a line like this, right. It's a red star or not. And then you do another one that's a blue circle or not. And then a third one, whether it's a green square or not.

Then you give that your unknown input, you ask all these three things, what do you think? So they'll say I think it's a red star or not, and then similarly for the other ones. Also remember, it'll also give you a number right, between zero and 1.

So you take the one that is the most [00:16:00]?? So if you get point 9 from the red, versus like point 4 or point 3 from the others, and then you would think, you would think that it's a red star, right. So that's multi-class, and right, so we'll get down to Ruby now, which is what you guys all love.

There is a very famous library called libsvm, which is available on the Unix [00:16:23]?? and there's a gem called rb - Ruby - libsvm, which shows how the code works, and that's what the code that I showed on GitHub is actually using that.

We will also talk about weka, which is a, some Java library, only good for ?? [00:16:38] and is awesome. I'll show you how we use that for Ruby.

And you could, of course, so [00:16:45] all the ?? is matrix or linear, so you could ?? you could find out either - or figure it out yourself, don't try to do that because it's very complicated, right. And there is - who has heard of Apache Mahout? That's probably very, very well well-known because it's highly scale-able but it's work setting up like a Apache Mahout and all that so I'll move on [00:17:09]??

Right, so using libsvm, and the exact code is in this model file, since we're still using Rails, right. So there you create a problem, and then you take in a parameter. And the parameter means things like using a linear or a parabola the shape of that line is called a curve, to put it simply.

So when I'm using a linear curve, or a polynomial or something like that, as you can see I'm just doing this most simple thing, which is taking the [00:17:41]?? and then our examples, which I showed you, is taking about 60,000 images, or I'm sorry, 60,000 items, where every item is again 28 by 28, so 785 numbers.

But every number is between zero and 255, right. So if you wrap around in SVM mode, feed it into our problem, as labels and features. Features are these guys, labels are the known labels, like is it the ones, is it the twos, things like that.

And then the training algorithm. The training takes some time, so usually it would say interrupt persisted, and then leave that data later on, and then use that to predict when you've been given it new data, like for example, my zero in my own handwriting, you take the array of 28 by 28 pixels, and you create the predict based on what it is, right.

Simple stuff. Right. 

Weka is another awesome tool, it's from a university in New Zealand's name I cannot pronounce well, it goes something like Waikato, I don't get it, anyway. So another label. Instead of showing the screenshots of this I'm going to show you Weka. Weka is awesome because it is so versatile, right. It comes with a [00:19:04]?? as well as a Java API. And I'll just show you the [00:19:07]?? part so. This is my training data, right, it reads something on the arff format, but it can read CSV also.

And I already read it in here, and it lets you picture like how your data loads like, first of all. So you can see I sent 4,784 images from that file, and it's pretty evenly distributed along the different classes and numbers, right, and I wanted to use something called Normalize, so that's a step where, so my numbers, attributes, go from zero to 255.

That's good, but, think about having, in the house pricing, from your one attribute which is square feet as 1,800, and the scale of thousands, and another attribute which uses the scale 1 or 2 or 5, right. So you will need to normalize it so every attribute is between a certain scale.

And Weka extracts that from any of the emergent sliders in the- that way, and you can see these are all my 784 attributes. And I will show you something in the middle. So you can see max is zero - min is zero, max is one, and how many mistake values and all that.

Most awesome part, and I'm going to say this multiple times, the most awesome part of Weka, is you choose any algorithm that you want, right. So you can do a layers/miles/files?? [00:20:33], someone of you may have heard this name, like a random format [00:20:36]?? or viewer samples?? and all that. You can pick any of that. And I pick the libSVM, which is the SVM.

And I'm saying, I'm gonna use 90% of the [00:20:46]?? instances to train it, and the rest can push it to test it on. And then it's going to build a model, and then it's going to also scale the model and all that, and more - but it will probably take about seventeen seconds, yes, six seconds, and then now it's testing on those instances, and soon it will report what the accuracy was.

This is probably not readable from the back, but I didn't build this software so don't worry ?? [00:21:13] 

91% accuracy, using SVM.

AUDIENCE MEMBER: [indecipherable - [00:21:18]]

And then this is like the builder feature, we already saw that. Almost everything is along the background, so that's good. And this is in part the best thing about Weka. It comes with something called the Experimenter, where you can feed it the data that you have, and you give it multiple algorithms, or the same number in different values and it's going to compare all this and tell you which is the best, it's pretty amazing, right.

So this is really, really awesome stuff. We use this a lot. It comes with a Java API, and this is how you use it. So yesterday Dejas told you about the best way to write Python. The best way to write Java is also in JRuby actually, so a class like this, Weka will classify something, becomes a new module. And Weka in some cases become this, and I'm so used to it I think that I hate [00:22:20]?? also.

Yeah. This is awesome.

So this is on the API, Java and initial data. So yeah, there are 70 options like linear, [00:22:28], the building it, [00:22:31]. Any time you have new data you classify it.

Right. And then we didn't really touch up on what we didn't do today, and there's a lot of stuff. This is just like the tip of the iceburg, this list is not complete. There's something called regularization where, if you have lots of points, like almost spread over it, and you, like your input is not even enough, it's going to create the curve that goes through it, every living example.

So when you have a Weka problem where your data is very similar to a [00:23:06] training?? example, it's going to predict exactly that data.

When it is, like, here or there, it's going to be completely wrong. So you need something called regularization to prevent that. It's not going to be very easy. We talked about normalization, we talked about polynomial classifiers. And I also think another thing which is sort of important, so we have 784 attributes in our problem. And it's print-line comes into like the end of of all this and all that, given 60,000 elements. So you do something called principle ?? analysis [00:23:40] which figures out which are important attributes are in this.

Because if you remember the image, the top, bottom, and the sides are all zero, right, almost always. So those are really not important to figure out what it is. And data generation. And so we won't talk about that today.

And then, if you're not inspired, go and take this class. This is where I started. Machine Learning Coursera class, it was the first Coursera class, pretty, pretty awesome. And it goes very deep, so that's good.

And then the collective intelligence in action is really, really good, and there's this, a website called kaggle, and you can, when you are reading about this stuff, they have something called competitions, and people like companies actually post their machine learning problems here. Right. And if you solve it then you get that much money. But there are lots of free competitions as well, where you can test out.

So this is one, Digit Register is one of those, and you don't get any money for that, but yeah. So that's pretty awesome, trying, you can try and go through that.

And that's it. Thank you.

V.O.: We have time for questions.

QUESTION: I have a question. Yeah. So in the case of when you are learning from this data, where do, how do you, how do you get known citation-[00:25:09] ?? 

A.D.: How do you find data?

QUESTION: How do you fix the emphasis of data, they have a lot of images, but only a few of them get passed in [00:25:19]. For example, if you take the example of the pricing of the flats-

A.D.: Right.

QUESTION: So each image the same- [00:25:23] depending on the same code, and they're probably offering at vastly different prices. And in such cases, [00:25:31]??

A.D.: So basically what you're saying is your data sort of implements, right.

That's probably not going to happen in the real world, where you probably have not found the right attributes, because, why is your neighbor giving you a flat fee in the first place [00:25:40], right. So that needs to be one of your attributes as well, right. But if there really, like, well simulated, then- So, what I showed was just the linear curve. You can have lots of polynomial , and by polynomial I mean weird shapes, so you could have, like, the center is that part, and again the outside part falls under one classification, the middle part does not, things like that.

So it's basically going to try out a lot of things, basically to figure out what is the algorithm that you need.

QUESTION: Can you, can you do the same pattern on any other media within the data, right? We have, like, extra data, like it's some extra number that is going into it. It is something [00:26:50]

A.D.: So the same way here, like we showed, what you are working with, right, images, right. Images, ultimately, just not ours, right, when it comes down to it. Similarly for for vice[00:27:01] for, either find the frequency or they, you know, amplitude, things like that, which I know I'm not gonna get into right now, but, you would find numerical fences at that points. And actually one of the widely-known uses of machine learning is filtering out speech from the noise in the background. Right. So that code data [00:27:22] you're trying to figure out my voice, versus, like if there's aircraft in the hangar. How do you get that out? So yeah.

You can [00:27:31].

QUESTION: Like if you [00:27:36]

A.D.: OK, so first of all this is a Ruby conference. So that's one big reason. Second thing is, when you're, when you're using libSVM, like you saw, and that's at the C level, so it doesn't really matter what I'm using. If you are using Weka, or using the JRuby, right. So at that point it doesn't really matter. I would still say that most people who are doing machine learning are not using Ruby, to be fair, because even the reading on the data might get, because you're working with Gigabytes or Terabytes of data, it might take awhile. But my point here is, you guys all know Ruby, or have visited Ruby, and this is another thing that I want to introduce you to, so this is probably the best way to learn it.

Because if you're going to learn like Julia or R along with all this, you're probably going to be a little lost. So it's maybe this could be your first step, trying it out, like with something you already know. Then progress if you get really interested in this stuff. So progress a lot more towards the tools you are going to use [00:28:51].

Any more questions?

That's it then.

V.O.: Thank you Ardak.

A.D.: Thank you.
