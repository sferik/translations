Garden City Ruby 2014 - I've Got Your Number: Machine Learning in Ruby - Arnab Deka

Hey guys. Good morning? Yeah.

So I'll talk about machine learning. The promise and all that was just to get the CFE expected, so - accepted, so. Don't expect too much.

Anyway, so I started with machine learning about six months to a year back, and this world like amazes me with how much stuff you can do and awesome it is. So at the end of this talk I just want you to get excited about that too and go and try it out.

You can read more about me there. Don't want to talk about that. The slides and the code that I'll show are up on GitHub and slides and all that.

Yeah, so I was preparing this thing and then Dejas, yesterday - everybody knows Dejas here, right? He had a couple of jokes that were exactly what I had, and I was like oh god I have to prepare new jokes now. But at night I couldn't so, it was too late anyway. So I didn't, so my jokes are still lame, but still laugh like you did for his talk, all right.

First of all, happy new year. Has anybody ever had two hundred people shout happy new year to you at the same time? No? Want to try that? All right, so, on a count of three, right.

One, two, three. Happy new year!

That was actually much better than what I expected. Cause I had two more steps after this. I'll just do one of them anyway. Let's try even better, right. Let the hotel know that we are doing something awesome in here.

So shout happy new year and stomp your feet. Don't worry, the floor is pretty strong, except probably this part. So, all right, like this. Three times, all right.

So one, two, three.

Happy new year!

All right. You guys are already awake I know but, anyway. So we'll take a very brief look at machine learning. And there are many awesome talks that tell you what machine learning can do. This is not what this talk will do.

I'll specifically focus on classification, which is one type of machine learning, and an algorithm that you can use to do that, which is SVM.

And then at the end we'll see how you can do this in Ruby.

So yeah. This is my first talk, and I thought I should do a live demo. Right? Before we actually go- so, what we are trying to do today is, we have all these large data set of about 60,000 images that look like that 6 and 5 and 7.

Each one is 28 by 28 pixels, and what we have is raw gray scale intensity. So you have zero to 255. And the, there's this data set available that you can use to practice your algorithm on, or train your algorithm on.

So it's hard to read, it's all binary and I don't understand binary. So I put it out in CSP like this, and as you can see most of the pixels here at the top, bottom, sides, they're all going to be zero, and in the middle you'll see some numbers that's going to range between zero and 255.

So what we're trying to do is, take these 60,000 images, basically create, have a algorithm learn those images, and then when we give it a new input, like another image, that sort of looks like one of these, or maybe a little dissimilar, it should be able to tell us what it is, right.

So that, the MNIST set also comes with 10,000 things for testing. So first we'll run it through that and see what our accuracy is, and then I'll show you some of my bad, bad handwriting. All right, so.

I will talk about most of these things later, but I'm basically - you don't need to know what linear is, I'm just showing that I'm doing it on 60,000 samples, and I won't do the all 10,000 right now I'll just do 500 of them.

And that's going to give, come back and say hey I'm 85% accuracy and I didn't train the algorithm right now because that takes a lot of time. I already have it saved. And you can see the code and all that, the saved models in GitHub, but for now the main important parts are this algorithm, it says it's 85% accurate, on the 500 samples that we tested.

And the, all these things that you see, like for example this guy, we got two images that are actually 9, but are reported as zero. Right, so those are bad. Everything that you see along the diagonal are good, everything else is bad. And I didn't tune the algorithm at all because I wanted to keep it simplistic and all that, but you could do a lot more stuff and as VM I think it has been shown that it can get about 93% accuracy with this.

All right, so. Another thing is, so I wrote down some of these numbers on a sticky note. That's probably not visible because it's all white. I'll see here. Right. So.

Something like this, right. They wrote it on sticky, and then scanned it in gray scale. Then I'm kind of going to read it from my own handwriting.

So I did, some of this Rails Magick stuff. Who's familiar with Rails Magick? Right, so I did some of that stuff to get the raw gray scale intensity on all that and so I made my picture look a little bit like normal gray scale. Right.

So yeah, I'm going to use this kind of - say zero is pretty easy for an example because not many shapes are like that. So let's see if it knows what's written. It says the answer is zero. That's good.

And I'm going to open this so I know which ones are written and which ones not.
[^ 00:06:09-00:06:12 - ??]

The one is working. And some of these are not working. For example I'll show you a 3. Look at it on the tables. That's 9. And then we'll get the 9. The way I write 9 is very similar to 3, except this part, right. So I'm going to try 9 and it's gonna tell me that it's a 3, right.

So that's when you need more accuracy to train your algorithm, right.

Anyway, I hope you sort of understand some what you can do with machine learning. So what we're doing here is, we're not explicitly programming the learning algorithm. We're just giving it a lot of data, and it automatically figures out how to solve a problem. Given unknown data.

Which is, I thought, amazing when I learned it. And it's basically statistics-driven algorithms to find patterns in data.

So there are two types of machine learning algorithms. The more widely-known ones are probably [00"07:12] house?? like supervised learning, where it has lots of data. For example, let's say you were doing house pricing, right. You have to have the area in square feet, you know the location of the house, you know how many bedrooms it has. Is it an apartment, does it have a swimming pool - things like that.

These are all attributes of our problem. When you know these attributes, you can basically, and without some set of data where you know the prices, you can now feed it to an algorithm, and when you have, I don't know, like, I just made up a two bedroom apartment, throw that in , what should be the price?

And it can give you the price, right. Of course it'll need a lot of work, but that's essentially what supervised learning is.

You are given some known data, and then you predict unknown data.

Unsupervised is, on the other hand, there's no, like, classified data or something like that. You just give it a lot of data, and you're trying to find patterns in that. And for example, recommendation engines, like Amazon or Flipkart - everybody - recommendation engines, right, they go do something like that.

Now Flipkart does do something like that [00:08:15]?? I'm guessing.

And they do finding groups or certain groups within your social network, things like that can be learned.

So today we'll just talk about classification. And, so that example I talked about house pricing. Let's say that we'll need one attribute, we're gonna, we can barely sample here. And can everybody see this side, or should I be talking on the other side of this room? This side is fine? This - that side?

All right. OK, so, just one attribute we are focusing on, and I'm plotting all these houses. So this much area, this is the price, this much area, this is the price, or something like that, right. And this is not exactly linear because there are other attributes. We are just considering one attribute.

And we have this red dot here that we want to predict the right price on. What do you think is the best way to do that?

AUDIENCE MEMBER: Extrapolation.

A.D.: Right, extrapolation. So this is a linear extrapolation, and it's finding a linear thing, the progression will give linear data. The dots will be here, there, and there, and in that case you look it up on the other [00:09:24]?? fitting curve. In this case it's a linear curve.

And then once it drew that, you figure out the dot will probably lie somewhere here. So it's running a price over here.

Which is not bad. [00:09:32]?? So how do you actually solve this problem? How do you find these dots that are away from the line? The thing you do - before I show you the math, for every one of these points, you say that if I knew what this quote was, how do I compare it to this?

So that's the lead hypothesis, right. 

Hypothesis on every dot is going to be this line equation here. You probably know this as V and this as M, right. V plus Mx. But I'm probably [00:10:11]?? not going to take that one, because you can have more than one attribute. You actually missed one. So how do we rotate out, concentrate and filter one, blah blah blah [00:10:20]??.

So you can't believe the hypothesis. Well right now you don't know the answer on data alone, right. So you know that Y there means is the actual price. So you take the Y and you find the difference between the hypothesis and the Y, while not moving the hypothesis, right? So you're just forming the formula. And then you try to minimize that part of the whole picture, right.

So what you do is a declaration for every point. You take the Y out, square it, because, and then it's absolute, and then you find the sum of it. And this is your minimization problem.

How many of you have heard about linear programming, or optimization minimization. Yeah, so this is basically a minimization problem, and I know this, we'll figure out what equals zero and we can go on [00:11:10]??.

And this was a linear thing, so linear, but [00:11:14]??

Next up is, that was linear, so the price is a real number, so you're trying to predict the exact ones, whereas this kind is saying is my- is this email spam or not spam?

And this is a very famous machine learning thing that Gmail?? [00:11:33]. So I'm going to keep it simple, just considering two attributes of email. Length of the email, and does it have a dollar symbol, right.

So based on those two attributes, I have plotted out, let's say something like - the red stars are spam emails. The blue ones are non-spam emails.

Now how do you get an unknown email, how do you classify whether it's a red star? It's very similar with the previous problem. You are again going to find the curve that's sort of a different, there's a difference between the red stars and not, and this one thing you do by computing some of these here [00:12:12]?? which both will show that this is probably not exactly a linear solution.

You'll need better than one attribute. So unlike the previous one, this time we need the hypothesis to be between zero and 1. So zero means it's not spam, and it doesn't matter what it means as long as they're consistent.

So the Y then will be either zero or one, so the hypothesis is also zero or one. And this is a very famous sigma function that I will deliver to you after you figure it out, but this for me ?? [00:12:46].

And here, like the previous one, we're just using equals zero plus either 1 [00:12:53] right. This side, that whole thing, is Z and Z here. And then you solve that minimizer equation I guarantee you'll find equals either zero or 1, and then you find that you are right where you should be.

So it will come somewhere between zero and 1, and you take that from. That's how you classify it.

Right. And how you exactly minimize those problems is, SVM is a powerful way to do that. Along with numeric constants and [00:13:21] to be used, machine learning and [00:13:23]. It's called a large margin classifier, and the SVM stands for Support Vector Machines. We'll see what [00:13:30] means.

When I started, I thought, why are these things formed the way they are here? So again, red stars and green stars, and we want to segregate them. We want to find the line that divides them. We can think of many lines. There's a pink line here, there's a light blue line there, then there's a deep blue line.

So, what it does [00:13:52] is it finds all the lines that it potentially can use, finds the critical points that matters as the distance between the [00:13:58] line and the blue??, right.

So in the case of the pink one, this is the closest one for this side, and that's probably, let me identify it, so one of these points is the closest - so this, the closest one, is called the support vector for that group, right. All these other points, the difference between the pink line, and all these other points around the blue line [00:14:25] are the minimal distance from the group.

So it finds out the line that best segregates these things. And this is the support vector, and this is the margin, the length of that is the margin, hence, large margin classifier, right.

So that's SVM, the big one, will get exactly the algorithm [00:14:45], right.

So, and if you're feeling that, like that, it's OK, because this is about six times the [00:14:54]??
