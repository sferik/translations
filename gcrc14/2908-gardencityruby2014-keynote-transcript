Garden City Ruby 2014 - Keynote - Chad Fowler

Chad: Yes, hello. Thank you.

Audience member: Hello!

Chad: Hello!

I am Chad, as he said. He said I need no introduction so I won't introduce myself any further. I may be the biggest non-Indian fan of India.

[Hindi speech]

I'll switch back now - sorry. If you don't understand Hindi, I said nothing of value and it was all wrong.

But I was saying that my Hindi was bad and it's because now I'm learning German, so I mixed them together. But I know not everyone speaks Hindi here. I just had to show off, you know.

So I am currently working on 6WunderKinder and I'm working on a product called Wunderlist. It is a productivity application. It runs on every client you can think of - we have native clients, we have a back-end, we have millions of active users.

And I'm telling you this not so that you'll go download it - you can do that too - but I want to tell you about the challenges that I have and the way I'm starting to think about system's architecture and design. 

That's what I'm gonna talk about today.

I'm going to show you some things that are real and that we're really doing. I'm going to show you some things that are just a fantasy that maybe don't make any sense at all. But hopefully I'll get you thinking about how we think about system architecture and how we build things that can last for a long time.

So the first thing I want to mention - this is a graph from the Standish Chaos report. I've taken the years out and I've taken some of the raw data out because it doesn't matter.

If you look at these, this graph, each one of these bars is a year. And each bar represents successful projects in green - software projects. Challenged projects are in silver or white in the middle and the failed ones are in red.

Challenged means significantly over time or budget, which to me means failed too.

So basically we're terrible. All of us here, we're terrible. We call ourselves engineers but it's a disgrace. We very rarely actually launch things that work.

Kind of sad. And I am here to bring you down.

Then, once you launch software, anecdotal-y, and you probably would see this in your own work lives too, anecdotal-y, software gets killed after about five years. Business software. 

So you barely ever get to launch it because - or at least successfully, in a way that you're proud of - and then in about five years you end up in that situation where you're doing a big rewrite and throwing everything away and replacing it. You know there's always that project to get rid of the junk, old Java code or whatever that you wrote five years ago.

Replace it with Ruby now. Five years from now you'll be replacing your old junk Ruby code that didn't work with something else. 

We create this thing, probably all of you know the term legacy software.

Right, am I right? You know what legacy software is.

And you probably think of it as a negative thing. You think of it as that ugly code that doesn't work, that's brittle, that you can't change, that you're all afraid of. 

But there's actually also a positive connotation of the word legacy. It's leaving behind something that future generations can benefit from.

But if we're rarely ever launching successful projects, and then the ones we do launch tend to die within five years, none of us are actually creating a legacy in our work. We're just creating stuff that gets thrown away.

Kind of sad.

So we create this stuff that's legacy software. It's hard to change - that's why it ends up getting thrown away.

Right, that's- if the software worked and you could keep changing it to meet the needs of the business, you wouldn't need to do a big rewrite and throw it away. We create these huge tightly-coupled systems, and I don't just mean one application, but like many applications are all tightly-coupled.

You've got this thing over here talking to the database of this system over here, so if you change the columns to update the view of a webpage you ruin your whole billing system. That kind of thing.

This is what makes it so hard to change, and the sad thing about this is the way we work, the way we develop software, this is the default setting. And what I mean is, if we were robots churning out software, and we had a preferences panel, the default preferences would lead to us creating terrible software that gets thrown away in five years.

That's just how we all work.

As human beings, when we sit down to write code, our default instincts lead us to create systems that are tightly coupled and hard to change and ultimately get thrown away and can't scale.

We create, we try doing tests, we try doing TDD, but we create test suites that take forty-five minutes to run. Every team has had to deal with this, I'm sure. If you've written any kind of meaningful application.

It gets to where you have, like, a project to speed up the test suite. Like you start focusing your company's resources on making the test suite faster, or making it only fail ninety percent of the time.

And then you say, well, if it only fails ninety percent that's OK, right. And right now it's taking forty-five minutes, we want to get it to where it only takes ten minutes to run. So the test suite ends up being a liability instead of a benefit because of the way you do it, because you have this architect where everything is so coupled.

You can't change anything without spending hours working on the stupid test suite, and you're terrified to deploy. I know, like the last big Java project I was working on - it would take, once a week we did a deploy. It would take fifteen people all night to deploy the thing, and usually it was like copying class files around and restarting servers.

It's much better today but it's still terrifying. You deploy code, you change it in production, you're not sure what might break, cause it's really hard to test these big, integrated things together. Actually upgrading the technology component is terrifying.

So, how many of you have been doing Rails for more than three years? Do you have a Rails 2 app in production? Anyone? Yeah?

That's a lot of people. Wow. That's terrifying.

And I've been in situations, recently, where we had Rails 2 apps in production. Security patches are coming out, we were applying our own versions of those security patches. Because we were afraid to upgrade Rails.

We would rather hack it than upgrade the thing, because you just don't know what's gonna happen, and then you end up, as you're re-implementing this stuff yourself, you end up burning yourself out, wasting your time, because you're hacking on stupid Rails 2 or some old struts version, when you should be just taking advantage of the new patches.

But you can't, because you're afraid to upgrade the software, because you don't know what's going to happen, because the system is too big and too scary.

Then, and this is really bad, I think this is something Ruby messes up for all of us - I say this as someone who's been using Ruby for thirteen years now, happily - we create these mountains of abstractions and the logic ends up being buried inside them.

I mean in Java it was like static or, you know, factories and design-pattern soup. In Ruby it's modules and mixins and, you know, we have all these crazy ways of hiding what's actually happening from us.

But when you go look at the code, it's completely opaque.

You have no idea where the stuff actually gets done because it's in some magic library somewhere, and we do all that because we're trying to save ourselves from the complexity of these big nasty systems.

But like, if you look at the rest of the world, this is a software-specific problem. These cars are old, they're older than any software that you would ever run, and they're still driving down the street. They're older than software itself, right, but these things still function, they still work.

How? Why? Why do they work?

Bodies!

My body should not work. I have abused it. I should not be standing here today. I shouldn't have been able to come from Berlin to here without dying somehow by being in the air, you know, by the air pressure changes.

But our bodies somehow can survive even when we don't take care of them, and like it's just the system that works, right?

So how do our bodies work? How do we stay alive despite this fact? Even though we haven't done like some great design, we don't have any design patterns like mixed up into our bodies.

In biology there is a term called homeostasis, and I literally don't know what this means other than this definition. So you won't learn about this from me - there's probably at least one biologist in the room, so you can correct me later.

But basically the idea of homeostasis is that an organism has all these different components that serve different purposes that regulate it. So they're all kind of in balance, and they work to regulate the system. If one component, like a liver, does too much or does the wrong thing, another component kicks in and fixes it.

So our bodies are this well-designed system for staying alive, because we have almost, like, autonomous agents internally that take care of the many things that can and do go wrong on a regular basis.

So you have, you know, your brain, your liver. Your liver of course metabolizes toxic substances. Your kidneys deal with blood, water level, et cetera. You know all these things work in concert to make you live.

The inability to continue to do that is known as homeostatic imbalance.

So I was saying homeostasis is balancing. Not being able to do that is when you're out of balance, and that will actually lead to really bad health problems, or probably death, if you fall into homeostatic imbalance.

So the good news is you're already dying. Like we're all dying all the time.

This is the beautiful thing about death. There is an estimate that fifty trillion cells are in your body, and three million die per second.

It's an estimate because it's actually impossible to count, but scientists have figured out somehow that this is probably the right number.

So your cells, you've probably heard this all your life, like physically, after some amount of time, you aren't the same human being you were physically.

You know, I don't know, you- some period of time ago- you're literally not the same organism anymore. But you're the same system.

Kind of interesting, isn't it.

So in a way you can think about software this- you can think about software as a system. If the components could be replaced like these cells, like, if you focus on making death, constant death, OK, on a small level, then the system can live on a large level.

That's what this talk is about.

Solution, the solution being to mimic living organisms.

And as an aside, I will say many times the word small or tiny in this talk, because I think I'm learning, as I age, that small is good.

Small projects are good - you know how to estimate them.

Small commitments are good, because you know you can make them.

Small methods are good. Small classes are good. Small applications are good. Small teams are good.

I don't know, this is sort of a non sequitur.

So, if we're going to think about software as like an organism, what is a cell in that context? This is sort of the key question that you have to ask yourself.

And I say that a cell is a tiny component.

Now, tiny and component are both subject words. So you can kind of do what you want with that. But it's a good frame of thinking.

If you make your software of tiny components, each one can be like a cell. Each one can die and the system is a collection of those tiny components, and what you want is not for your code to live forever, you don't care that each line of code lives forever, right.

Like if you're trying to develop a legacy in software, it's not important to you that your system dot out dot printline statement lives for ten years. It's important to you that the function of the system lives for ten years.

So, like, about exactly ten years ago, we created Ruby gems at the RubyConf 2003 in Austin, Texas. I haven't touched Ruby gems myself in like four or five years, but people are still using it. They hate it because it's software - everybody hates software, right. So if you create software that people hate you've succeeded.

But it still exists. I have no idea if any of the code is the same - I would assume not. You know, I think, I'm sure that my name is still in it in a copyright notice, but that's about it. That's a beautiful thing.

People are still using it to install Ruby libraries and software. And I don't care if any of my existing, or my initial code is still in the system, because the system still lives.

So quite a long time ago now I was researching this kind of question about legacy software, and I asked a question on Twitter as I often do at conferences when I'm preparing.

What are some of the old surviving software systems you regularly use? And if you look at this, I mean, one thing is obvious: everyone who answered gave some sort of Unix-related answer. But basically all of these things on the list are either systems that are collections of really well-known split-up components or they're tiny, tiny programs.

So like Grep is a tiny program, and Make. It only does one thing.

Well Make is actually also arguably an operating system, but I won't get into that.

Emacs is obviously an operating system, right.

But it's well-designed of these tiny little pieces. So a lot of the old systems I know about follow this pattern, this metaphor that I'm proposing. And from my own career, when I was here before in Banglore, I worked for GE and some of the people we hired even worked on the system here.

We had a system called the Bull, and it was a Honeywell Bull mainframe. I doubt any of you have worked on that, but this one I know you didn't work on, because it had a custom operating system with our own RDVMS.

We had created a PCP stack for it, using like custom hardware that we plugged into a Windows MT computer, with some sort of MT queuing system back in the day.

It was this terrifying thing. When I started working there, the system was already something like twenty-five years old, and I believe even though there have been many, many projects to try to kill it - like we had a team called the Bull Exit team - I believe the system is still in production. Not as much as it used to be, there are less and less functions in production. But I believe the system is still in production.

The reason for this is that the system was actually made up of these tiny little components, and like really queer interfaces between them. We kept the system live because every time we tried to replace it with some fancy new gem, web thing or gooey app, it wasn't as good and the users hated it.

It just didn't work.

So we had to use this old, crazy, modified mainframe for a long time as a result. So the question I ask myself is now, how do I approach a problem like this and build a system that can survive for a long time?

I would encourage you-

How many of you know of Fred George? This is Fred George. He was at Thought Works for awhile, so he may have, I think he lived in Banglore for some time with Thought Works, in fact.

He is now running a start-up in Silicon Valley, but he has this talk that you can watch online from the Barcelona Ruby conference the year before last called Microservice Architectures. He talks in great detail about how he implemented a concept at Forward that's very much like what I'm talking about.

Tiny components that do one thing and can be thrown away.

So Microservice Architecture is kind of the core of what I'm gonna talk about.

Now I've put together some rules for 6WunderKinder which I am going to share with you.

6WunderKinder is the company I work for when we're working on Wunderlist.

And the rules of the- the goals of these rules are to reduce coupling, to make it where we can do fear-free deployments. We reduce the chance of 'cruft' in our code, like nasty stuff that you're afraid of, that if you leave there, kind of broken window problems.

We make it literally trivial to change code. So you just never have to ask, how do I do that? You just find it easy.

Most importantly we give ourselves the freedom to go fast, because I think no developer ever wants to be slow. That's one of the worst things, just toiling away and not actually accomplishing anything. But we go slow because we're constrained by the system and we're constrained by sometimes projects and other, you know, management related things.

But often times it's the mess of the system that we've created.

So some of the rules.

I think one thing, and maybe I'm going to get some push back from this crowd, one rule that is less controversial than it used to be is that comments are a design smell.

Does anyone strongly disagree with that?

No?

Does anyone strongly agree with that?

OK, so the rest of you have no idea what I'm talking about.

So a design smell, I want to define this really quickly. A design smell is something you see in your code or your system, where it doesn't necessarily mean it's bad, but you look at it and you think, hmm, I should look into this a little bit and ask myself, why are there so many comments in this code?

You know, especially the bottom one - inline comments? Definitely bad, definitely a sign that you should have another method, right.

So it's pretty easy to convince people that comments are a design smell and I think a lot of people in the industry are starting to agree.

Maybe not for like a public library, where you really need to tell someone, here's how you use this class and this is what it's for, but you shouldn't have to document every method and every argument because the method name and the argument name should speak for themselves, right.

So here's one that you probably won't agree with. Tests are a design smell.

So this one is probably a little more controversial, especially in an environment where you're maybe still struggling with people to actually get them to write tests to begin with, right.

You know, I went through this period in like 200 and 2001 where I was really heavily into evangelizing TDD. It was really stressful that you couldn't get anyone to do it. I think you do have to go through that period and I'm not saying you shouldn't write any tests.

But that picture I showed you earlier of the slow, brittle test suite - that's bad, right. That is a bad state to be in, and you're in that state because your tests suck.

That's why you get in that state. Your tests suck because you're writing bad tests that don't exercise the right things in your system, and what I've found whenever I look into one of these big slow brittle test suites: the tests themselves are indications, and the sheer proliferation of tests, are indications that the system is bad.

The developers are like desperately, fearfully trying to run the code in every way they can, because it's the only way they can manage to even think about the complexity.

But if you think about it, if you had a tiny, trivial system, you wouldn't need to have hundreds of test files that take ten minutes to run, ever.

If you did, you're doing something stupid. You're wasting your time working on tests and we as software developers obsess about this kind of thing, because we have to fight so hard to get our peers to do it in the first place and to understand it.

We obsess to the point where we focus on the wrong thing.

None of us are in the business of writing tests for customers. Like we're not launching our tests to the web and hoping people will buy them, right.

It doesn't provide value, it's just a side-effect that we have focused too heavily on and we've lost sight of what the actual goal is.

So this one actually requires a visual. I tell the people on my team now, you can write code in any language you want, any framework you want, anything you want to do, as long as the code is this big.

So if you want to write the new service in Heskell and it's this big in a normal-sized font, you can do it.

If you want to do it in Closure or Elixir or Scarla or Ruby or whatever you want to do - even Python, for god's sake - you can do it if it's this big and no bigger.

Why? Because it means I can look at it and understand it, or if I don't I'll just throw it away because if it's this big it doesn't do very much, right. So the risk is really low.

And I really mean the system is that, there are the... the component is that big, and in my world a component means a service that's running and probably listening on an HTTP board or some sort of rift or RPC protocol.

So it's a standalone thing. It's its own application. It's probably in its own git repository. People do poll requests against it.

But it's just tiny - so this big.

At the top of this, by the way, is some code written by Konstantin Haase, who also lives in Berlin where I live.

This is a rewrite of Sinatra, the web framework, and Konstantin is actually the maintainer of Sinatra. It's not fully compatible, but it's amazingly close and it all fits right in that.

But the font size is kind of small, so I cheated.

Another rule-

Our system is heterogeneous by default. So I say you can write in any language you want, that's not just because I want the developers to be excited.

Although I think, most of you, if you worked in an environment where your boss told you you can use any programming language or tool you want, you would be pretty happy about that, right.

Anyone unhappy about that? I don't think so.

Unless it's one of the bosses here that's like, don't tell people that. So that's one thing.

The other one is, it leads to a good system design, because, think about this:

If I write one program in Erlang, one component in Erlang, one program in Ruby, I have to work really, really hard to make tight coupling between those things.

Like, I have to basically use computer science to do that. I don't even know what I would do.

You know it's hard. Like I would have to maybe implement Ruby in Erlang so that it can run in the same BM or vice versa. It's just silly. I wouldn't do it.

So if my system is heterogeneous by default, my coupling is very low, at least at a certain level, by default, because the path of least resistance is to make the system decoupled if they're all running in different languages.

So in the past three months, I'll say, I've written production code in objective CRuby, Scala, Closure, Node. I don't know, more stuff - Java, all these different languages.

Real code for work. And yes, they are not tightly coupled.

Like I haven't installed JRuby so that I could reach into the internals of my Scala code, because that would be a pain. I don't want to do that.

Another very important one is server nodes are disposable.

So, back when I was at GE, for example, I remember being really proud when I looked up the up time of one of my servers and it was like four hundred days or something.

It's like, wow, this is awesome. I have this big server, it had all these apps on it, we kept it running for four hundred days.

The problem with that is I was afraid to ever touch it. I was really happy it was alive but I didn't want to do anything to it.

I was afraid to update the operating system. In fact, you could not upgrade Solaris then without restarting it, so that meant I had not been upgrading the operating system.

I probably shouldn't have been too proud about it.

Nodes that are alive for a long time lead to fear, and what I want is less fear. So I throw them away, and this means-

I don't have physical servers that I throw away. That would be fun but I'm not that rich yet.

We use AWS right now, you could do it with any kind of cloud service or even internal cloud divider.

But every node is disposable, so we never upgrade software on an existing server. Whenever you want to deploy a new version of a service, you create new servers and you deploy that version, and then you replace in them in load balance or somewhere. That's it.

So, you never have to wonder what's on a server because it was deployed through an automated process and there's no fear there.

You know exactly what it is. You know exactly how to recreate it because you have a golden master image, and in our case it's actually an Amazon image that you can just boot more of.

Scaling is a problem. You just boot ten more servers - boom, done, no problem.

So, yeah, I tell the team, you know, pick your technology.

Everything must be automated, that's another piece. If you're going to deploy a closure service for the first time you have to be responsible for figuring out how it fits into our deployment system so that you have the immutable deployments and disposable nodes.

If you can do that and you're willing to also maintain it and teach someone else about the little piece of code that you wrote, then cool. You can do it, any level you want.

And then once you deploy stuff, like a lot of us like to just SFH in the machines, and then twiddle with things and replace files and like try fixing bugs live on production.

Why not just throw away the actual keys, because you're going to throw away the system eventually. You don't even need route access to it. You don't need to be able to get to it except through the port that your service is listening on.

So you can't screw it up. You can't introduce entropy and mess things up if you throw away the keys.

So this is actually a practice that you can do. Deploy the servers, remove all the credentials for logging in, and the only option you have is to destroy them when you're done with them.

Provisioning services in our new world must also be trivial. So we actually now throw away our chef repository because chef is obsolete and we have replaced it with shell scripts.

And that sounds like I'm an idiot, I know, but when I say Chef is obsolete, I don't really mean that. I like to say that so that people will think, because a lot of you are probably thinking, we should move to Chef. That would be great.

Because what you have is a bunch of servers that are running for a long time and you need to be able to continue to keep them up to date. Chef is really great at that.

Chef is also good at booting a new server, but really it's just overkill for that. Yeah.

So if you're always throwing stuff away, I don't think you need Chef.

Do something really, really simple and that's what we've done.

So like whenever we deploy a new type of service-

I set up ZooKeeper recently, which is a complete change from the other stuff we're deploying.

I think it was a five line Shell script to do that. I just added it to a get repo and run a command.

I've got a cluster of ZooKeeper servers running.

You want to always be deploying your software. This is something I learned from Kent Beck early on in the Agile extreme programming world, that if something is hard, or you perceive it to be hard or difficult, the best thing you can do, if you have to do that thing all the time, is to just do it constantly, non-stop, all the time.

So like deploying in our old world, where it would take all nice once a week if we instituted a new policy in that team, said any change that goes to master must be deployed within five minutes. I guarantee you we would have fixed that process, right.

And if you're deploying constantly, all day every day, you're never going to be afraid of deployments because it's always a small change.

So always be deploying.

Every new deploy means you're throwing away old servers and replacing them with new ones. In our world I would say that the average up time of one of our servers is probably something like seventeen hours, and that's because we don't tend to work on the weekend very much.

You also, when you have these sorts of systems that are distributed like this, and you're trying to reduce the fear of change, the big thing that you're afraid of is failure. You're afraid that the service is going to fail, the system is going to go down, one component won't be reachable, that sort of thing.

So you just have to assume that that's going to happen. You are not going to build a system that never fails, ever.

I hope you don't, because you will have wasted much of your life trying to get that to happen.

Instead, assume that the thing, the components are going to fail, and build resiliency in.

I have a picture here of Joe Armstrong, who is one of the inventors of Erlang.

If you have not studied Erlang's philosophy around failure and recovery you should. It won't take you long so I'm just going to leave that as homework for you.

And then, you know, I said the tests are a design smell.

I don't mean don't write any tests, but I also want to be further responsible here and say you should monitor everything. You want to favor measurement over testing, so I use measurement as a surrogate for testing, or as an enhancement.

And the reason I say this is you can either focus on one of two things.

I said assume failure, right, so:

Mean time between failures or mean time to resolution.

Those are kind of two metrics in the ops world that people talk about for measuring their success and their effectiveness.

Mean time between failures means you're trying to increase the time between failures of the system, so basically you're trying to make failures never happen, right.

Mean time to resolution means when they happen, I'm gonna focus on bringing them back as fast as I possibly can.

So a perfect example would be a system fails and another one is already up and just takes over its work.

Mean time to resolution is essentially zero, right.

If you're always assuming that every component can and will fail then mean time resolution is going to be really good because you're going to bake it into the process.

If you do that, you don't care about when things fail.

And back to this idea of favoring measurement over testing. If you're monitoring everything with intelligence, then you're actually focusing on mean time to resolution and acknowledging that the software is going to be broken sometimes, right.

And when I say monitor everything, I mean everything. I don't mean, like your disk space and your memory and stuff here.

I'm talking about business metrics.

So at Living Social we created this thing called Rearview which is now OpenSource, which allows you to do aberration detection.

And aberration means strange behavior, strange change in behavior. So Rearview can do aberration detection on data sets, arbitrary data sets, which means-

Like in our Living Social world, we had user sign ups constantly streaming in. It was a very high volume site. If user sign ups were weird we would get an alert.

Why might they be weird?

One thing that could be like the user service is down, right, so then we would get two alerts.

User sign ups have gone down and so has the service.

So obviously the problem is the service is down. Let's bring it back up.

But it could be something like a front-end developer or a designer made a change that was intentional, but it just didn't work and no one liked it. So they didn't sign up to the site anymore.

That's more important than just knowing that the service is down, right, because what you care about isn't that the service is up or down.

If you could crash the entire system and still be making money you don't care, right, that's better. Throw it away and stop paying for the servers.

But if your system is up 100% of the time and performs excellently but no one's using it, that's bad.

So monitoring business metrics gives you a lot more than unit test could ever give you.

And then in our world we're focused on experience-

[Interruption]

No, you have to come up to front and say ten!

OK, ten minutes left.

When I got to 6WunderKinder in Berlin everyone was terrified to touch the system because they had created a really well-designed but traditional monolithic API.

So they had layers of abstractions. It was all kind of in one big thing.

They had a huge database and they were really, really scared to do anything.

So there's like one person who would deploy anything and everyone else was trying to work on other projects and not touch it.

But it was like the production system, you know, so it wasn't really an option.

So the first thing I did in my first week is I got these graphs going and this was, yeah, response time. And the first thing I did is I started turning off servers and just watching the graphs.

And then, as I was turning off the servers, I went to the production database and I did select, count, star from tasks, and we're a task management app so we have hundreds of millions of tasks, and the whole thing crashed.

And all the people were like, ah! What's going on?

You know, and I said, it's no problem. I did this on purpose, I'll just make it come back, which I did.

And from that point on, like, really every day I would do something which basically crashed the system for just a moment. And really, like, we had way too many servers in production. We were spending tends of thousands more Euros per month than we should have on the infrastructure.

And I just started taking things away, and I would usually do it, instead of the responsible way like one server at a time, I would just remove all of them and start adding them back. So for a moment everything was down.

But after that we got to a point where everyone on the team was absolutely comfortable with the worst case scenario of the system being completely down, so that we could, in a panic-free way, just focus on bringing it up when it was bad.

So now when you do a deployment and you have your business metrics being measure, you know the important stuff is happening and you know what to do when everything is down.

You've experienced the worst thing that can happen.

Well, the worst thing is like, someone breaks in and steals all your stuff, steals all your users' phone numbers and posts them online like SnapChat or something.

But you've experienced all these potentially horrible things and realized, eh, it's not so bad, I can deal with this, I know what to do.

It allows you to start making bold moves and that's what we all want, right. We all want to be able to bravely go into our systems and do anything we think is right.

So that's what I've been focusing on.

We also do this thing called canary in the coalmine deployments, which removes the fear also.

Canary in the coalmine refers to a kind of sad thing about coal miners in the US, where they would send canaries into the mines at various levels, and if the canary died they knew there was a problem with the air.

But in the software world what this means is you have a bunch of servers running, or a bunch of, I don't know, clients running a certain version, and you start introducing new versions incrementally and watching the effects.

So once you're measuring everything and monitoring everything, you can also start doing these canary in the coalmine things, where you say, OK, I have a new version of this service and I'm going to deploy, and I've got about thirty servers running for it.

I'm going to change only five of them now and see, like, does my error rate increase, or does my performance drop on those servers, or do people actually not successfully complete the task they're trying to do on those servers.

So this also allows us the combination of monitoring everything and these immutable deployments and everything, gives us the ability to gradually effect change and not be afraid.

So we roll out changes all day every day because we don't fear that we're just going to destroy the entire system all at once.

So I think I have like five minutes left.

These are some things that we're not necessarily doing yet, but they're some ideas that I have that, given some free time, I will work on.

And they're probably more exciting.

One is, I talked about homeostatic regulation and homeostasis.

So I think we all understand the idea of, you know, homeostasis and the fact that systems have different parts that do different roles and can protect each other from each other.

So this diagram is actually just some random diagram I copied and pasted off the AWS website, so it's not necessarily all that meaningful except to show that every architecture, especially server-based architectures, has a collection of services that play different roles, and it almost looks like a person.

You've got a brain and a heart and a liver and all these things, right.

What would it mean to actually implement homeostatic regulation in a web service, so that you have some controlling system where the database will actually kill an app server that is hurting it, for example - just kill it.

I don't know yet, I don't know what that, but some ideas about this stuff-

I don't know if you've heard of these. Netflix, do you have Netflix in India yet?

Probably not, unless you have a VPN, right. Netflix has a really great cloud-based architecture. They have this thing called Chaos Monkey they've created, which goes through their system and randomly destroys nodes, just crashes servers, and they did this because when they were-

They were early users of AWS, and when they went out initially with AWS, servers were crashing, like it was still immature. So they said, OK, we still want to use this, and we'll build in stuff so that we can deal with the crashes, but we have to know it's gonna work when it crashes.

So let's make crashing be a part of production. So they actually have gotten really sophisticated now and they will crash entire regions cause they're in multiple data centers.

So they'll say like, what would happen if this data center went down, does the site still stay up? And they do this in production all the time, like they're crashing servers right now. It's really neat.

Another one that is inspirational in this way is Pinterest. They use AWS as well, and they have-

AWS has this thing called Spot Instances, and I won't go into too much detail because I don't have time, but Spot Instances allow you to effectively bid on servers at a price that you are willing to pay.

So like if a usual server costs twenty cents per minute, you can say, I'll give you fifteen cents per minute, and when excess capacity comes open, it's almost like a stock market.

If fifteen cents is the going price, you'll get a server, and it starts up and it runs what you want.

But here's the cool thing. If the stock market goes and the price goes higher than you're willing to pay, Amazon will just turn off those servers. They're just dead, you don't have any warning. They're just dead.

So Pinterest uses this for their production servers, which means they save a lot of money. They're paying way under the average Amazon cost for hosting, but the really cool thing, in my opinion, is not the money they save but the fact that, like, what would you have to do to build a full system where any node can and will die at any moment, and it's not even under your control.

That's really exciting.

So a simple thing you can do for homeostasis, though, is you can just adjust. So in our world we have multiple nodes and all these little services.

We can scale each one independently, we're measuring everything.

So Amazon has a thing called Auto Scaling, we don't use it, we do our own scaling, we just do it based on volume and performance. Now when you have a bunch of services like this, like, I don't know, maybe we have fifty different devices now that each play tiny little roles, it becomes difficult to figure out, like, where things are.

So we've started implementing ZooKeeper for service resolution, which means a service can come online and say, I'm the reminder service version 2.3, and then tell a central guardian, and the ZooKeeper can then route traffic to it.

Probably too detailed for now.

I'm gonna skip over some stuff real quick, but I want to talk about this one.

If, did the Nordic Ruby, no, Nordic Ruby talks never go online.

So you can never see this talk, sorry.

At Nordic Ruby, Reginald Braitwaite did a really cool talk on like the challenges of the Ruby languages, and he made this statement:

Ruby has some beautiful but static coupling, which was really strange.

But basically he was making the same point that I was talking about earlier, that, like, Ruby creates a bunch of ways that you can couple your system together that kind of screw you in the end.

But they're really beautiful to use. But, like, Ruby can lead to some deep crazy coupling, and so he presented this idea of bind by contract.

Bind by contract, in a Ruby sense, would be, like, I have a class that has a method that takes these parameters under these conditions and I can kind of put it into my VM, and whenever someone needs to have a functionality like that it will be automatically bound together by the fact that it can do that thing.

And instead of how we tend to use Ruby and Java and other languages, I have a class with a method name I'm going to call it, right, that's coupling.

But he proposed this idea of the decoupled system, where you just say, I need a functionality like this that works under the conditions that I have present.

So this lead me to this idea, and this may be like, way too weird, I don't know.

What if in your web application your route files for your services read like a functional pattern matching syntax?

So like, if you've ever used Erlang or Haskell or Scala, any of these things that have functional pattern matching, what if you could then route to different services, across a bunch of different services, based on contract.

Now I have zero time left, but I'm just gonna keep talking, cause I'm mean.

Oh wait, I'm not allowed to be mean because of the code of conduct, so I'll wrap up.

So this is an idea that I've started working on as well, where I would actually write an Erlang service with this sort of functional pattern matching but have it be routing in really fast real time through back-end services that support it.

One more thing I just want to show you real quick that I am working on and I want to show you because I want you to help me.

Has anyone used JSON Schema?

OK, you people are my friends for the rest of the conference.

In a system where you have all these things talking to each other, you do need a way to validate the inputs and outputs, but I don't want to generate code that parses and creates JSON.

I don't want to do something in real time that intercepts my kind of traffic, so there's this thing called JSON Schema that allows you to, in a completely decoupled way, specify JSON documents and how they should interact, and I am working on a new thing that's called Klagen - which is the German word for complain.

It's written in Scala, so if anyone wants to pair up on some Scala stuff, what it will be is a high performance asynchronous JSON Schema validation middleware.

So if that's interesting to anyone, even if you don't know Scala or JSON Schema, please let me know.

And I believe I'm out of time so I'm just gonna end there.

Am I right? I'm right, yes.

So thank you very much, and let's talk during the conference.
