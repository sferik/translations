Garden City Ruby 2014 - I've Got Your Number: Machine Learning in Ruby - Arnab Deka

ARNAB DEKA: Hey guys.

Good morning? Yeah. So I'll talk about machine learning.

The promise and all that was just to get

the CFE expected, so - accepted, so. Don't expect

too much. Anyway, so I started with machine learning

about six months to a year back, and this

world like amazes me with how much stuff you

can do and awesome it is. So at the

end of this talk I just want you to

get excited about that too and go and try

it out. You can read more about me there.

Don't want to talk about that. The slides and

the code that I'll show are up on GitHub

and slides and all that. Yeah, so I was

preparing this thing and then Dejas, yesterday - everybody

knows Dejas here, right? He had a couple of

jokes that were exactly what I had, and I

was like oh god I have to prepare new

jokes now. But at night I couldn't so, it

was too late anyway. So I didn't, so my

jokes are still lame, but still laugh like you

did for his talk, all right. First of all,

happy new year. Has anybody ever had two hundred

people shout happy new year to you at the

same time? No? Want to try that? All right,

so, on a count of three, right. One, two,

three. Happy new year! That was actually much better

than what I expected. Cause I had two more

steps after this. I'll just do one of them

anyway. Let's try even better, right. Let the hotel

know that we are doing something awesome in here.

So shout happy new year and stomp your feet.

Don't worry, the floor is pretty strong, except probably

this part. So, all right, like this. Three times,

all right. So one, two, three. Happy new year!

All right. You guys are already awake I know

but, anyway. So we'll take a very brief look

at machine learning. And there are many awesome talks

that tell you what machine learning can do. This

is not what this talk will do. I'll specifically

focus on classification, which is one type of machine

learning, and an algorithm that you can use to

do that, which is SVM. And then at the

end we'll see how you can do this in

Ruby. So yeah. This is my first talk, and

I thought I should do a live demo. Right?

Before we actually go- so, what we are trying

to do today is, we have all these large

data set of about 60,000 images that look like

that 6 and 5 and 7. Each one is

28 by 28 pixels, and what we have is

raw gray scale intensity. So you have zero to

255. And the, there's this data set available that

you can use to practice your algorithm on, or

train your algorithm on. So it's hard to read,

it's all binary and I don't understand binary. So

I put it out in CSP like this, and

as you can see most of the pixels here

at the top, bottom, sides, they're all going to

be zero, and in the middle you'll see some

numbers that's going to range between zero and 255.

So what we're trying to do is, take these

60,000 images, basically create, have a algorithm learn those

images, and then when we give it a new

input, like another image, that sort of looks like

one of these, or maybe a little dissimilar, it

should be able to tell us what it is,

right. So that, the MNIST set also comes with

10,000 things for testing. So first we'll run it

through that and see what our accuracy is, and

then I'll show you some of my bad, bad

handwriting. All right, so. I will talk about most

of these things later, but I'm basically - you

don't need to know what linear is, I'm just

showing that I'm doing it on 60,000 samples, and

I won't do the all 10,000 right now I'll

just do 500 of them. And that's going to

give, come back and say hey I'm 85% accuracy

and I didn't train the algorithm right now because

that takes a lot of time. I already have

it saved. And you can see the code and

all that, the saved models in GitHub, but for

now the main important parts are this algorithm, it

says it's 85% accurate, on the 500 samples that

we tested. And the, all these things that you

see, like for example this guy, we got two

images that are actually 9, but are reported as

zero. Right, so those are bad. Everything that you

see along the diagonal are good, everything else is

bad. And I didn't tune the algorithm at all

because I wanted to keep it simplistic and all

that, but you could do a lot more stuff

and as VM I think it has been shown

that it can get about 93% accuracy with this.

All right, so. Another thing is, so I wrote

down some of these numbers on a sticky note.

That's probably not visible because it's all white. I'll

see here. Right. So. Something like this, right. They

wrote it on sticky, and then scanned it in

gray scale. Then I'm kind of going to read

it from my own handwriting. So I did, some

of this Rails Magick stuff. Who's familiar with Rails

Magick? Right, so I did some of that stuff

to get the raw gray scale intensity on all

that and so I made my picture look a

little bit like normal gray scale. Right. So yeah,

I'm going to use this kind of - say

zero is pretty easy for an example because not

many shapes are like that. So let's see if

it knows what's written. It says the answer is

zero. That's good. And I'm going to open this

so I know which ones are written and which

ones not. [^ 00:06:09-00:06:12 - ??] The one is

working. And some of these are not working. For

example I'll show you a 3. Look at it

on the tables. That's 9. And then we'll get

the 9. The way I write 9 is very

similar to 3, except this part, right. So I'm

going to try 9 and it's gonna tell me

that it's a 3, right. So that's when you

need more accuracy to train your algorithm, right. Anyway,

I hope you sort of understand some what you

can do with machine learning. So what we're doing

here is, we're not explicitly programming the learning algorithm.

We're just giving it a lot of data, and

it automatically figures out how to solve a problem.

Given unknown data. Which is, I thought, amazing when

I learned it. And it's basically statistics-driven algorithms to

find patterns in data. So there are two types

of machine learning algorithms. The more widely-known ones are

probably [00"07:12] house?? like supervised learning, where it has

lots of data. For example, let's say you were

doing house pricing, right. You have to have the

area in square feet, you know the location of

the house, you know how many bedrooms it has.

Is it an apartment, does it have a swimming

pool - things like that. These are all attributes

of our problem. When you know these attributes, you

can basically, and without some set of data where

you know the prices, you can now feed it

to an algorithm, and when you have, I don't

know, like, I just made up a two bedroom

apartment, throw that in , what should be the

price? And it can give you the price, right.

Of course it'll need a lot of work, but

that's essentially what supervised learning is. You are given

some known data, and then you predict unknown data.

Unsupervised is, on the other hand, there's no, like,

classified data or something like that. You just give

it a lot of data, and you're trying to

find patterns in that. And for example, recommendation engines,

like Amazon or Flipkart - everybody - recommendation engines,

right, they go do something like that. Now Flipkart

does do something like that [00:08:15]?? I'm guessing. And

they do finding groups or certain groups within your

social network, things like that can be learned. So

today we'll just talk about classification. And, so that

example I talked about house pricing. Let's say that

we'll need one attribute, we're gonna, we can barely

sample here. And can everybody see this side, or

should I be talking on the other side of

this room? This side is fine? This - that

side? All right. OK, so, just one attribute we

are focusing on, and I'm plotting all these houses.

So this much area, this is the price, this

much area, this is the price, or something like

that, right. And this is not exactly linear because

there are other attributes. We are just considering one

attribute. And we have this red dot here that

we want to predict the right price on. What

do you think is the best way to do that?

AUDIENCE MEMBER: Extrapolation.

A.D.: Right, extrapolation. So this

is a linear extrapolation, and it's finding a linear

thing, the progression will give linear data. The dots

will be here, there, and there, and in that

case you look it up on the other [00:09:24]??

fitting curve. In this case it's a linear curve.

And then once it drew that, you figure out

the dot will probably lie somewhere here. So it's

running a price over here. Which is not bad.

[00:09:32]?? So how do you actually solve this problem?

How do you find these dots that are away

from the line? The thing you do - before

I show you the math, for every one of

these points, you say that if I knew what

this quote was, how do I compare it to

this? So that's the lead hypothesis, right.

00:09:58
